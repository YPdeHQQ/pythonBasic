# _*_ coding: utf-8 _*_
# @Time: 2023/5/12 15:40
# @Author: ğŸˆ
# @File: çˆ¬è™«è°ƒåº¦å™¨

from SpiderProjects.DataOutput import DataOutput
from SpiderProjects.HtmlDownloader import HtmlDownloader
from SpiderProjects.HtmlParser import HtmlParser
from SpiderProjects.UrlManager import UrlManager


class SpiderMan(object):

    def __init__(self):
        self.manager = UrlManager()
        self.downloader = HtmlDownloader()
        self.parser = HtmlParser()
        self.output = DataOutput()

    def crawl(self, root_url):
        # æ·»åŠ å…¥å£url
        self.manager.add_new_url(root_url)
        # åˆ¤æ–­urlç®¡ç†å™¨ä¸­æ˜¯å¦æœ‰æ–°çš„url, åŒæ—¶åˆ¤æ–­æŠ“å–äº†å¤šå°‘ä¸ªurl
        while self.manager.has_new_url() and self.manager.old_url_size()<100:
            try:
                # ä»urlç®¡ç†å™¨è·å–æ–°çš„url
                new_url = self.manager.get_new_url()
                # HTMLä¸‹è½½å™¨ä¸‹è½½ç½‘é¡µ
                html = self.downloader.download(new_url)
                # HTMLè§£æå™¨æŠ½å–ç½‘é¡µæ•°æ®
                new_urls, data = self.parser.parser(new_url, html)
                # å°†æŠ½å–çš„urlæ·»åŠ åˆ°urlç®¡ç†å™¨ä¸­
                self.manager.add_new_urls(new_urls)
                # æ•°æ®å­˜å‚¨å™¨å­˜å‚¨æ–‡ä»¶
                self.output.store_data(data)
                print('å·²ç»æŠ“å– %s ä¸ªæ•°æ®' %self.manager.old_url_size())
            except Exception as e:
                print('crawl failed')
        self.output.output_html()


if __name__ == '__main__':
    spider_man = SpiderMan()
    spider_man.crawl("https://baidu.com")